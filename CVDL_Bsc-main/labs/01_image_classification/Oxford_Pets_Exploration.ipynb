{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2570d29",
   "metadata": {},
   "source": [
    "# Exploration of the Oxford Pets Dataset\n",
    "\n",
    "In this notebook, we load and explore the Oxford Pets dataset using the Hugging Face `datasets` library. We perform the following steps:\n",
    "\n",
    "- Load the dataset and select relevant columns (`img` and `class`).\n",
    "- Apply a transformation to convert images from PIL format to NumPy arrays.\n",
    "- Analyze image resolution statistics (width and height).\n",
    "- Visualize the distribution of image widths and heights.\n",
    "- Analyze the distribution of class labels.\n",
    "- Provide initial insights and observations about the dataset.\n",
    "- **Additional Exploration:**\n",
    "    - Analyze aspect ratios of the images.\n",
    "    - Compute channel-wise statistics (mean and standard deviation) for normalization.\n",
    "    - Preview a simple data augmentation (horizontal flip) that may be applied during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define a transformation function to convert images to NumPy arrays\n",
    "def transform(example):\n",
    "    # Convert the PIL Image in 'img' to a NumPy array\n",
    "    example[\"img\"] = np.array(example[\"img\"])\n",
    "    return example\n",
    "\n",
    "# Load the Oxford Pets dataset from Hugging Face\n",
    "print('Loading Oxford Pets dataset...')\n",
    "ds = load_dataset(\"cvdl/oxford-pets\")\n",
    "\n",
    "# Select only the 'img' and 'class' columns\n",
    "print('Selecting columns...')\n",
    "ds = ds.select_columns([\"img\", \"class\"])\n",
    "\n",
    "# Apply the transformation to each example\n",
    "print('Applying transformation...')\n",
    "ds = ds.with_transform(transform)\n",
    "\n",
    "# Display available dataset splits and their sizes\n",
    "print(\"Dataset splits:\", list(ds.keys()))\n",
    "for split in ds.keys():\n",
    "    print(f\"Number of examples in {split} split: {len(ds[split])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6356eace",
   "metadata": {},
   "source": [
    "## Resolution Analysis\n",
    "\n",
    "Next, we analyze the resolution of the images in the **train** split. We extract the width and height of each image and compute basic statistics such as the average, minimum, and maximum dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c57e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'train' split for detailed analysis\n",
    "train_dataset = ds[\"train\"]\n",
    "\n",
    "# Initialize lists to store image widths and heights\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "# Extract image resolutions\n",
    "for sample in train_dataset:\n",
    "    img = sample[\"img\"]\n",
    "    h, w = img.shape[0], img.shape[1]  # (height, width)\n",
    "    widths.append(w)\n",
    "    heights.append(h)\n",
    "\n",
    "widths = np.array(widths)\n",
    "heights = np.array(heights)\n",
    "\n",
    "# Compute resolution statistics\n",
    "total_images = len(widths)\n",
    "avg_width = np.mean(widths)\n",
    "avg_height = np.mean(heights)\n",
    "min_width = np.min(widths)\n",
    "max_width = np.max(widths)\n",
    "min_height = np.min(heights)\n",
    "max_height = np.max(heights)\n",
    "\n",
    "print(\"=== Resolution Statistics for Train Split ===\")\n",
    "print(f\"Total number of images: {total_images}\")\n",
    "print(f\"Average width: {avg_width:.1f} pixels\")\n",
    "print(f\"Average height: {avg_height:.1f} pixels\")\n",
    "print(f\"Minimum width: {min_width} pixels, Minimum height: {min_height} pixels\")\n",
    "print(f\"Maximum width: {max_width} pixels, Maximum height: {max_height} pixels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b91377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of image widths and heights\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(widths, bins=20, alpha=0.5, label='Width')\n",
    "plt.hist(heights, bins=20, alpha=0.5, label='Height')\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Image Widths and Heights (Train Split)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676349cb",
   "metadata": {},
   "source": [
    "## Label Distribution Analysis\n",
    "\n",
    "We also analyze the distribution of class labels in the **train** split. This helps in identifying class imbalances and understanding the overall structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47cb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute label (class) distribution\n",
    "label_counts = {}\n",
    "for sample in train_dataset:\n",
    "    label = sample[\"class\"]\n",
    "    label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "print(\"=== Label Distribution in Train Split ===\")\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    print(f\"Class {label}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92834712",
   "metadata": {},
   "source": [
    "## Observations and Insights\n",
    "\n",
    "1. **Image Resolutions:** The dataset exhibits a wide range of image resolutions. This variability suggests that images were captured under different conditions or devices, and it indicates that careful preprocessing (e.g., resizing) may be needed.\n",
    "\n",
    "2. **Image Composition:** While many images focus on the pets, others include more background. This diversity in composition might affect model performance and should be considered when designing the training pipeline.\n",
    "\n",
    "3. **Label Information:** The dataset includes class labels that are essential for classification tasks. Analyzing the label distribution is important to address potential class imbalances.\n",
    "\n",
    "4. **Preprocessing Considerations:** Given the variability in resolutions and compositions, steps such as normalization, resizing, and data augmentation will be important for effective model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f86f4",
   "metadata": {},
   "source": [
    "## Additional Data Exploration for Model Training\n",
    "\n",
    "In this section, we further explore the data to extract details that are important for model training. We will:\n",
    "\n",
    "- Analyze the **aspect ratio** distribution of the images.\n",
    "- Compute **channel-wise statistics** (mean and standard deviation) for normalization.\n",
    "- Preview a simple **data augmentation** (horizontal flip) to understand its effect on the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8fdb0f",
   "metadata": {},
   "source": [
    "### Aspect Ratio Analysis\n",
    "\n",
    "The aspect ratio (width/height) can be important when deciding how to resize or crop images for model training. Let's compute and visualize the distribution of aspect ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08c992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute aspect ratios for images in the train split\n",
    "aspect_ratios = widths / heights\n",
    "\n",
    "print(\"=== Aspect Ratio Statistics ===\")\n",
    "print(f\"Average aspect ratio: {np.mean(aspect_ratios):.2f}\")\n",
    "print(f\"Minimum aspect ratio: {np.min(aspect_ratios):.2f}\")\n",
    "print(f\"Maximum aspect ratio: {np.max(aspect_ratios):.2f}\")\n",
    "\n",
    "# Plot histogram of aspect ratios\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(aspect_ratios, bins=20, alpha=0.7)\n",
    "plt.xlabel('Aspect Ratio (Width/Height)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Image Aspect Ratios (Train Split)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea53f9",
   "metadata": {},
   "source": [
    "### Channel-wise Statistics\n",
    "\n",
    "For training deep learning models, it is common to normalize the images using the mean and standard deviation computed over the dataset. Below, we compute these statistics for the RGB channels using a random subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b4508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# To reduce computation time, sample a subset of images from the train split\n",
    "sample_size = 100\n",
    "sample_indices = random.sample(range(len(train_dataset)), sample_size)\n",
    "\n",
    "channel_sums = np.zeros(3)\n",
    "channel_squared_sums = np.zeros(3)\n",
    "pixel_count = 0\n",
    "\n",
    "for idx in sample_indices:\n",
    "    img = train_dataset[idx][\"img\"]\n",
    "    # Ensure the image has 3 channels (RGB)\n",
    "    if img.ndim == 3 and img.shape[2] == 3:\n",
    "        pixel_count += img.shape[0] * img.shape[1]\n",
    "        channel_sums += img.sum(axis=(0, 1))\n",
    "        channel_squared_sums += (img ** 2).sum(axis=(0, 1))\n",
    "\n",
    "mean = channel_sums / pixel_count\n",
    "std = np.sqrt(channel_squared_sums / pixel_count - mean ** 2)\n",
    "\n",
    "print(\"=== Channel-wise Statistics (computed on a subset) ===\")\n",
    "print(f\"Mean per channel: {mean}\")\n",
    "print(f\"Standard deviation per channel: {std}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f8640a",
   "metadata": {},
   "source": [
    "### Data Augmentation Preview: Horizontal Flip\n",
    "\n",
    "Data augmentation is an essential step to improve model generalization. Here, we preview a simple horizontal flip augmentation by displaying the original image alongside its horizontally flipped version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a61d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "num_samples = 3\n",
    "for i in range(num_samples):\n",
    "    # Randomly select an image from the train dataset\n",
    "    sample = train_dataset[random.randint(0, len(train_dataset) - 1)]\n",
    "    original_img = sample[\"img\"]\n",
    "    # Create a horizontally flipped version using NumPy\n",
    "    flipped_img = np.fliplr(original_img)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original_img)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(flipped_img)\n",
    "    plt.title(\"Horizontally Flipped\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81f73e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "- Loaded and transformed the Oxford Pets dataset for easier exploration.\n",
    "- Analyzed image resolution statistics and label distribution.\n",
    "- Explored additional factors for model training such as aspect ratios and channel-wise statistics, which can be used for image normalization.\n",
    "- Previewed a simple data augmentation technique (horizontal flip) that can help improve model robustness.\n",
    "\n",
    "These insights will be useful for designing a preprocessing pipeline and training an effective model on the Oxford Pets dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b33703",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
