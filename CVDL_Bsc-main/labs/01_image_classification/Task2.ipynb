{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task II: Familiarizing Yourself with the Dataset (Oxford Pets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Data Overview\n",
    "The Oxford-IIIT Pet Dataset contains 37 breeds of cats and dogs, with:\n",
    "\n",
    "- 3680 images\n",
    "- Class labels corresponding to pet breeds\n",
    "- Varying image resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, scipy, datasets\n",
    "\n",
    "print(f\"numpy version: {numpy.__version__}\")\n",
    "print(f\"scipy version: {scipy.__version__}\")\n",
    "print(f\"datasets version: {datasets.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Load Oxford Pets dataset from torchvision\n",
    "# dataset = torchvision.datasets.OxfordIIITPet(\n",
    "#     root=\"./data\",\n",
    "#     download=True,\n",
    "#     transform=transforms.ToTensor()\n",
    "# )\n",
    "\n",
    "# # Create a DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Check dataset size\n",
    "# print(f\"Loaded {len(dataset)} images from Oxford Pets dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset details\n",
    "# print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load the Oxford Pets Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load both training and test datasets\n",
    "train_dataset = torchvision.datasets.OxfordIIITPet(\n",
    "    root=\"./data\",\n",
    "    split=\"trainval\",  # Training + Validation set\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.OxfordIIITPet(\n",
    "    root=\"./data\",\n",
    "    split=\"test\",  # Test set\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training Set Size: {len(train_dataset)} images\")\n",
    "print(f\"Test Set Size: {len(test_dataset)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Check Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check one sample\n",
    "sample_img, sample_label = train_dataset[0]\n",
    "\n",
    "# Print details\n",
    "print(f\"Sample Image Shape: {sample_img.shape}\")  # Tensor format: (C, H, W)\n",
    "print(f\"Sample Label: {sample_label}\")  # Label as an integer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Images have 3 color channels (RGB).\n",
    "- Different resolutions (no fixed width/height).\n",
    "- Labels are integer class indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4) Display Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to display images\n",
    "def show_images(dataset, num_images=5):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        img, label = dataset[i]  # Get image and label\n",
    "        axes[i].imshow(img.permute(1, 2, 0))  # Convert (C, H, W) to (H, W, C)\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"Class: {label}\")\n",
    "    plt.show()\n",
    "\n",
    "# Show sample images\n",
    "show_images(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Analyze Image Sizes (Resolution)\n",
    "- Images have different resolutions â†’ Need resizing before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get image sizes (width, height)\n",
    "image_sizes = [train_dataset[i][0].shape[1:] for i in range(len(train_dataset))]  # (H, W)\n",
    "image_sizes = np.array(image_sizes)\n",
    "\n",
    "# Compute average size\n",
    "avg_width = np.mean(image_sizes[:, 1])\n",
    "avg_height = np.mean(image_sizes[:, 0])\n",
    "\n",
    "print(f\"Average Image Size: {avg_width:.2f} x {avg_height:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Check Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get all labels\n",
    "train_labels = [train_dataset[i][1] for i in range(len(train_dataset))]\n",
    "\n",
    "# Count class occurrences\n",
    "label_counts = Counter(train_labels)\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(label_counts.keys(), label_counts.values())\n",
    "plt.xlabel(\"Class Index\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.title(\"Class Distribution in Oxford Pets Dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
